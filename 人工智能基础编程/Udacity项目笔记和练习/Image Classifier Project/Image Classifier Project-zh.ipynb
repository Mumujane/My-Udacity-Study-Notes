{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports here\n",
    "import json\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sb\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开发 AI 应用\n",
    "\n",
    "未来，AI 算法在日常生活中的应用将越来越广泛。例如，你可能想要在智能手机应用中包含图像分类器。为此，在整个应用架构中，你将使用一个用成百上千个图像训练过的深度学习模型。未来的软件开发很大一部分将是使用这些模型作为应用的常用部分。\n",
    "\n",
    "在此项目中，你将训练一个图像分类器来识别不同的花卉品种。可以想象有这么一款手机应用，当你对着花卉拍摄时，它能够告诉你这朵花的名称。在实际操作中，你会训练此分类器，然后导出它以用在你的应用中。我们将使用[此数据集](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html)，其中包含 102 个花卉类别。你可以在下面查看几个示例。 \n",
    "\n",
    "<img src='assets/Flowers.png' width=500px>\n",
    "\n",
    "该项目分为多个步骤：\n",
    "\n",
    "* 加载和预处理图像数据集\n",
    "* 用数据集训练图像分类器\n",
    "* 使用训练的分类器预测图像内容\n",
    "\n",
    "我们将指导你完成每一步，你将用 Python 实现这些步骤。\n",
    "\n",
    "完成此项目后，你将拥有一个可以用任何带标签图像的数据集进行训练的应用。你的网络将学习花卉，并成为一个命令行应用。但是，你对新技能的应用取决于你的想象力和构建数据集的精力。例如，想象有一款应用能够拍摄汽车，告诉你汽车的制造商和型号，然后查询关于该汽车的信息。构建你自己的数据集并开发一款新型应用吧。\n",
    "\n",
    "首先，导入你所需的软件包。建议在代码开头导入所有软件包。当你创建此 notebook 时，如果发现你需要导入某个软件包，确保在开头导入该软件包。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据\n",
    "\n",
    "在此项目中，你将使用 `torchvision` 加载数据（[文档](http://pytorch.org/docs/master/torchvision/transforms.html#)）。数据应该和此 notebook 一起包含在内，否则你可以[在此处下载数据](https://s3.amazonaws.com/content.udacity-data.com/nd089/flower_data.tar.gz)。数据集分成了三部分：训练集、验证集和测试集。对于训练集，你需要变换数据，例如随机缩放、剪裁和翻转。这样有助于网络泛化，并带来更好的效果。你还需要确保将输入数据的大小调整为 224x224 像素，因为预训练的网络需要这么做。\n",
    "\n",
    "验证集和测试集用于衡量模型对尚未见过的数据的预测效果。对此步骤，你不需要进行任何缩放或旋转变换，但是需要将图像剪裁到合适的大小。\n",
    "\n",
    "对于所有三个数据集，你都需要将均值和标准差标准化到网络期望的结果。均值为 `[0.485, 0.456, 0.406]`，标准差为 `[0.229, 0.224, 0.225]`。这样使得每个颜色通道的值位于 -1 到 1 之间，而不是 0 到 1 之间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'flowers'\n",
    "# 训练集，，需要进行变换数据，比如剪裁图片、随机缩放和对图片进行翻转。\n",
    "train_dir = data_dir + '/train'\n",
    "# 验证集， 用来衡量模型对未知数据的预测效果。\n",
    "valid_dir = data_dir + '/valid'\n",
    "# 测试集， 只需对图像大小进行核算的裁剪，不需要对其中的图像作任何缩放或选中变换。\n",
    "test_dir = data_dir + '/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此处用来定义训练集、验证集和测试集的转换方式\n",
    "# TODO: Define your transforms for the training, validation, and testing sets\n",
    "data_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                     transforms.RandomHorizontalFlip(0.25),\n",
    "                                     transforms.RandomRotation(30),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize( (0.485, 0.456, 0.406),\n",
    "                                                           (0.229, 0.224, 0.225))])\n",
    "valid_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                     transforms.RandomHorizontalFlip(0.25),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize( (0.485, 0.456, 0.406),\n",
    "                                                           (0.229, 0.224, 0.225))])\n",
    "test_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
    "                                     transforms.RandomHorizontalFlip(0.25),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize( (0.485, 0.456, 0.406),\n",
    "                                                           (0.229, 0.224, 0.225))])\n",
    "\n",
    "# TODO: Load the datasets with ImageFolder\n",
    "train_datasets = datasets.ImageFolder(train_dir, transform=data_transforms)\n",
    "valid_datasets = datasets.ImageFolder(valid_dir, transform=data_transforms)\n",
    "test_datasets = datasets.ImageFolder(test_dir, transform=data_transforms)\n",
    "\n",
    "# TODO: Using the image datasets and the trainforms, define the dataloaders\n",
    "trainloader = torch.utils.data.DataLoader(train_datasets, batch_size=64, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_datasets, batch_size=32, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_datasets, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 标签映射\n",
    "\n",
    "你还需要加载从类别标签到类别名称的映射。你可以在文件 `cat_to_name.json` 中找到此映射。它是一个 JSON 对象，可以使用 [`json` 模块](https://docs.python.org/2/library/json.html)读取它。这样可以获得一个从整数编码的类别到实际花卉名称的映射字典。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cat_to_name.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 构建和训练分类器\n",
    "\n",
    "数据准备好后，就开始构建和训练分类器了。和往常一样，你应该使用 `torchvision.models` 中的某个预训练模型获取图像特征。使用这些特征构建和训练新的前馈分类器。\n",
    "\n",
    "这部分将由你来完成。如果你想与他人讨论这部分，欢迎与你的同学讨论！你还可以在论坛上提问或在工作时间内咨询我们的课程经理和助教导师。\n",
    "\n",
    "请参阅[审阅标准](https://review.udacity.com/#!/rubrics/1663/view)，了解如何成功地完成此部分。你需要执行以下操作：\n",
    "\n",
    "* 加载[预训练的网络](http://pytorch.org/docs/master/torchvision/models.html)（如果你需要一个起点，推荐使用 VGG 网络，它简单易用）\n",
    "* 使用 ReLU 激活函数和丢弃定义新的未训练前馈网络作为分类器\n",
    "* 使用反向传播训练分类器层，并使用预训练的网络获取特征\n",
    "* 跟踪验证集的损失和准确率，以确定最佳超参数\n",
    "\n",
    "我们在下面为你留了一个空的单元格，但是你可以使用多个单元格。建议将问题拆分为更小的部分，并单独运行。检查确保每部分都达到预期效果，然后再完成下个部分。你可能会发现，当你实现每部分时，可能需要回去修改之前的代码，这很正常！\n",
    "\n",
    "训练时，确保仅更新前馈网络的权重。如果一切构建正确的话，验证准确率应该能够超过 70%。确保尝试不同的超参数（学习速率、分类器中的单元、周期等），寻找最佳模型。保存这些超参数并用作项目下个部分的默认值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.torch/models/densenet121-a639ec97.pth\n",
      "100%|██████████| 32342954/32342954 [00:00<00:00, 77065947.23it/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Build and train your network\n",
    "# 设置GPU\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 1: 构建网络\n",
    "model = models.densenet121(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseNet(\n",
       "  (features): Sequential(\n",
       "    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu0): ReLU(inplace)\n",
       "    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (denseblock1): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition1): _Transition(\n",
       "      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock2): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition2): _Transition(\n",
       "      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock3): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer17): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer18): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer19): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer20): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer21): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer22): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer23): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer24): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (transition3): _Transition(\n",
       "      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "    )\n",
       "    (denseblock4): _DenseBlock(\n",
       "      (denselayer1): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer2): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer3): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer4): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer5): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer6): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer7): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer8): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer9): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer10): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer11): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer12): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer13): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer14): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer15): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "      (denselayer16): _DenseLayer(\n",
       "        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu1): ReLU(inplace)\n",
       "        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu2): ReLU(inplace)\n",
       "        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      )\n",
       "    )\n",
       "    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (classifier): classifier(\n",
       "    (hidden_layers): ModuleList(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.5)\n",
       "    (output): Linear(in_features=512, out_features=102, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置参数\n",
    "for param in model.parameters():\n",
    "    param.requires_grad= False\n",
    "\n",
    "# 分类器超参数\n",
    "input_size = 1024\n",
    "hidden_layers = [512]\n",
    "output_size = 102\n",
    "drop_p = 0.5\n",
    "epochs = 30\n",
    "\n",
    "# 添加自己的分类器\n",
    "class classifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_layers, drop_p):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        \n",
    "        if len(hidden_layers) != 0:\n",
    "            layers = zip(hidden_layers[:-1], hidden_layers[1:])\n",
    "            self.hidden_layers.extend([nn.Linear(h1, h2) for h1, h2 in layers])\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=drop_p)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_layers[-1], output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for linear in self.hidden_layers:\n",
    "            x = F.relu(linear(x))\n",
    "            x = self.dropout(x)\n",
    "            \n",
    "        x = self.output(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model.classifier = classifier(input_size, output_size, hidden_layers, drop_p)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证试验\n",
    "def validation(model, criterion, validloader):\n",
    "    loss = 0\n",
    "    accuracy = 0\n",
    "    for images, targets in iter(validloader):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        output = model.forward(images)\n",
    "        loss += criterion(output, targets).item()\n",
    "        \n",
    "        ps = torch.exp(output)\n",
    "        equality = (targets.data == ps.max(dim=1)[1])\n",
    "        accuracy += equality.type(torch.FloatTensor).mean()\n",
    "        \n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: 训练网络\n",
    "# criterion & optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义训练模型\n",
    "def train(model, trainloader, validloader, criterion, optimizer, print_step=32, epochs=30):\n",
    "    \n",
    "    # training\n",
    "    steps = 0\n",
    "    running_loss = 0\n",
    "    print_step = 32\n",
    "    \n",
    "    # validation result variables\n",
    "    vloss = 0\n",
    "    vaccuracy = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for e in range(epochs):\n",
    "        for images, labels in iter(trainloader):\n",
    "            steps += 1\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model.forward(images)\n",
    "            loss = criterion(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if steps%print_step == 0:\n",
    "\n",
    "                model.eval()\n",
    "\n",
    "                changed = False\n",
    "\n",
    "                if steps%(10*print_step) == 0:\n",
    "                    with torch.no_grad():\n",
    "                        vloss, vaccuracy = validation(model, criterion, validloader)\n",
    "                        changed = True\n",
    "\n",
    "                end=time.time()\n",
    "\n",
    "                change = \"(c)\" if changed else \"\"\n",
    "\n",
    "                print('Epoch: {}/{}\\t'.format(e+1, epochs),\n",
    "                     'Train Loss: {:.3f}\\t'.format(running_loss/print_step),\n",
    "                     'Valid Loss: {:.3f}\\t'.format(vloss/len(validloader)),\n",
    "                     'Valid Accuracy: {:.3f}\\t'.format(vaccuracy/len(validloader)*100),\n",
    "                     'Step Time: {:.2f}  {}'.format(end-start, change))\n",
    "                start = end\n",
    "                running_loss = 0\n",
    "\n",
    "                model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/30\t Train Loss: 4.412\t Valid Loss: 0.000\t Valid Accuracy: 0.000\t Step Time: 37.83  \n",
      "Epoch: 1/30\t Train Loss: 3.791\t Valid Loss: 0.000\t Valid Accuracy: 0.000\t Step Time: 37.52  \n",
      "Epoch: 1/30\t Train Loss: 3.179\t Valid Loss: 0.000\t Valid Accuracy: 0.000\t Step Time: 37.66  \n",
      "Epoch: 2/30\t Train Loss: 2.568\t Valid Loss: 0.000\t Valid Accuracy: 0.000\t Step Time: 33.81  \n",
      "Epoch: 2/30\t Train Loss: 2.236\t Valid Loss: 0.000\t Valid Accuracy: 0.000\t Step Time: 33.85  \n",
      "Epoch: 2/30\t Train Loss: 1.955\t Valid Loss: 0.000\t Valid Accuracy: 0.000\t Step Time: 33.79  \n",
      "Epoch: 3/30\t Train Loss: 1.766\t Valid Loss: 0.000\t Valid Accuracy: 0.000\t Step Time: 33.28  \n",
      "Epoch: 3/30\t Train Loss: 1.650\t Valid Loss: 0.000\t Valid Accuracy: 0.000\t Step Time: 33.89  \n",
      "Epoch: 3/30\t Train Loss: 1.522\t Valid Loss: 0.000\t Valid Accuracy: 0.000\t Step Time: 33.83  \n",
      "Epoch: 4/30\t Train Loss: 1.468\t Valid Loss: 1.135\t Valid Accuracy: 74.273\t Step Time: 47.87  (c)\n",
      "Epoch: 4/30\t Train Loss: 1.396\t Valid Loss: 1.135\t Valid Accuracy: 74.273\t Step Time: 33.87  \n",
      "Epoch: 4/30\t Train Loss: 1.274\t Valid Loss: 1.135\t Valid Accuracy: 74.273\t Step Time: 33.99  \n",
      "Epoch: 5/30\t Train Loss: 1.291\t Valid Loss: 1.135\t Valid Accuracy: 74.273\t Step Time: 33.27  \n",
      "Epoch: 5/30\t Train Loss: 1.286\t Valid Loss: 1.135\t Valid Accuracy: 74.273\t Step Time: 33.88  \n",
      "Epoch: 5/30\t Train Loss: 1.110\t Valid Loss: 1.135\t Valid Accuracy: 74.273\t Step Time: 33.99  \n",
      "Epoch: 5/30\t Train Loss: 1.181\t Valid Loss: 1.135\t Valid Accuracy: 74.273\t Step Time: 33.88  \n",
      "Epoch: 6/30\t Train Loss: 1.117\t Valid Loss: 1.135\t Valid Accuracy: 74.273\t Step Time: 33.55  \n",
      "Epoch: 6/30\t Train Loss: 1.155\t Valid Loss: 1.135\t Valid Accuracy: 74.273\t Step Time: 34.01  \n",
      "Epoch: 6/30\t Train Loss: 0.989\t Valid Loss: 1.135\t Valid Accuracy: 74.273\t Step Time: 33.82  \n",
      "Epoch: 7/30\t Train Loss: 1.049\t Valid Loss: 0.787\t Valid Accuracy: 80.523\t Step Time: 46.16  (c)\n",
      "Epoch: 7/30\t Train Loss: 1.045\t Valid Loss: 0.787\t Valid Accuracy: 80.523\t Step Time: 33.85  \n",
      "Epoch: 7/30\t Train Loss: 1.018\t Valid Loss: 0.787\t Valid Accuracy: 80.523\t Step Time: 33.78  \n",
      "Epoch: 8/30\t Train Loss: 1.012\t Valid Loss: 0.787\t Valid Accuracy: 80.523\t Step Time: 33.17  \n",
      "Epoch: 8/30\t Train Loss: 0.911\t Valid Loss: 0.787\t Valid Accuracy: 80.523\t Step Time: 33.93  \n",
      "Epoch: 8/30\t Train Loss: 0.960\t Valid Loss: 0.787\t Valid Accuracy: 80.523\t Step Time: 33.89  \n",
      "Epoch: 9/30\t Train Loss: 0.925\t Valid Loss: 0.787\t Valid Accuracy: 80.523\t Step Time: 33.33  \n",
      "Epoch: 9/30\t Train Loss: 0.948\t Valid Loss: 0.787\t Valid Accuracy: 80.523\t Step Time: 33.81  \n",
      "Epoch: 9/30\t Train Loss: 0.910\t Valid Loss: 0.787\t Valid Accuracy: 80.523\t Step Time: 33.72  \n",
      "Epoch: 10/30\t Train Loss: 0.897\t Valid Loss: 0.787\t Valid Accuracy: 80.523\t Step Time: 33.29  \n",
      "Epoch: 10/30\t Train Loss: 0.905\t Valid Loss: 0.724\t Valid Accuracy: 81.288\t Step Time: 47.14  (c)\n",
      "Epoch: 10/30\t Train Loss: 0.830\t Valid Loss: 0.724\t Valid Accuracy: 81.288\t Step Time: 33.84  \n",
      "Epoch: 10/30\t Train Loss: 0.913\t Valid Loss: 0.724\t Valid Accuracy: 81.288\t Step Time: 33.89  \n",
      "Epoch: 11/30\t Train Loss: 0.869\t Valid Loss: 0.724\t Valid Accuracy: 81.288\t Step Time: 33.08  \n",
      "Epoch: 11/30\t Train Loss: 0.871\t Valid Loss: 0.724\t Valid Accuracy: 81.288\t Step Time: 33.91  \n",
      "Epoch: 11/30\t Train Loss: 0.883\t Valid Loss: 0.724\t Valid Accuracy: 81.288\t Step Time: 33.91  \n",
      "Epoch: 12/30\t Train Loss: 0.879\t Valid Loss: 0.724\t Valid Accuracy: 81.288\t Step Time: 33.23  \n",
      "Epoch: 12/30\t Train Loss: 0.823\t Valid Loss: 0.724\t Valid Accuracy: 81.288\t Step Time: 33.70  \n",
      "Epoch: 12/30\t Train Loss: 0.855\t Valid Loss: 0.724\t Valid Accuracy: 81.288\t Step Time: 33.87  \n",
      "Epoch: 13/30\t Train Loss: 0.842\t Valid Loss: 0.724\t Valid Accuracy: 81.288\t Step Time: 33.35  \n",
      "Epoch: 13/30\t Train Loss: 0.870\t Valid Loss: 0.644\t Valid Accuracy: 82.401\t Step Time: 46.82  (c)\n",
      "Epoch: 13/30\t Train Loss: 0.791\t Valid Loss: 0.644\t Valid Accuracy: 82.401\t Step Time: 33.69  \n",
      "Epoch: 14/30\t Train Loss: 0.764\t Valid Loss: 0.644\t Valid Accuracy: 82.401\t Step Time: 33.20  \n",
      "Epoch: 14/30\t Train Loss: 0.781\t Valid Loss: 0.644\t Valid Accuracy: 82.401\t Step Time: 33.92  \n",
      "Epoch: 14/30\t Train Loss: 0.820\t Valid Loss: 0.644\t Valid Accuracy: 82.401\t Step Time: 33.92  \n",
      "Epoch: 14/30\t Train Loss: 0.764\t Valid Loss: 0.644\t Valid Accuracy: 82.401\t Step Time: 33.94  \n",
      "Epoch: 15/30\t Train Loss: 0.797\t Valid Loss: 0.644\t Valid Accuracy: 82.401\t Step Time: 33.10  \n",
      "Epoch: 15/30\t Train Loss: 0.724\t Valid Loss: 0.644\t Valid Accuracy: 82.401\t Step Time: 33.88  \n",
      "Epoch: 15/30\t Train Loss: 0.828\t Valid Loss: 0.644\t Valid Accuracy: 82.401\t Step Time: 33.93  \n",
      "Epoch: 16/30\t Train Loss: 0.796\t Valid Loss: 0.644\t Valid Accuracy: 82.401\t Step Time: 33.23  \n",
      "Epoch: 16/30\t Train Loss: 0.769\t Valid Loss: 0.574\t Valid Accuracy: 84.767\t Step Time: 47.07  (c)\n",
      "Epoch: 16/30\t Train Loss: 0.791\t Valid Loss: 0.574\t Valid Accuracy: 84.767\t Step Time: 34.00  \n",
      "Epoch: 17/30\t Train Loss: 0.800\t Valid Loss: 0.574\t Valid Accuracy: 84.767\t Step Time: 33.11  \n",
      "Epoch: 17/30\t Train Loss: 0.768\t Valid Loss: 0.574\t Valid Accuracy: 84.767\t Step Time: 33.78  \n",
      "Epoch: 17/30\t Train Loss: 0.796\t Valid Loss: 0.574\t Valid Accuracy: 84.767\t Step Time: 33.76  \n",
      "Epoch: 18/30\t Train Loss: 0.713\t Valid Loss: 0.574\t Valid Accuracy: 84.767\t Step Time: 32.90  \n",
      "Epoch: 18/30\t Train Loss: 0.753\t Valid Loss: 0.574\t Valid Accuracy: 84.767\t Step Time: 33.76  \n",
      "Epoch: 18/30\t Train Loss: 0.692\t Valid Loss: 0.574\t Valid Accuracy: 84.767\t Step Time: 33.82  \n",
      "Epoch: 19/30\t Train Loss: 0.762\t Valid Loss: 0.574\t Valid Accuracy: 84.767\t Step Time: 33.22  \n",
      "Epoch: 19/30\t Train Loss: 0.738\t Valid Loss: 0.574\t Valid Accuracy: 84.767\t Step Time: 34.00  \n",
      "Epoch: 19/30\t Train Loss: 0.725\t Valid Loss: 0.633\t Valid Accuracy: 83.129\t Step Time: 47.27  (c)\n",
      "Epoch: 19/30\t Train Loss: 0.779\t Valid Loss: 0.633\t Valid Accuracy: 83.129\t Step Time: 34.02  \n",
      "Epoch: 20/30\t Train Loss: 0.729\t Valid Loss: 0.633\t Valid Accuracy: 83.129\t Step Time: 33.16  \n",
      "Epoch: 20/30\t Train Loss: 0.723\t Valid Loss: 0.633\t Valid Accuracy: 83.129\t Step Time: 33.77  \n",
      "Epoch: 20/30\t Train Loss: 0.704\t Valid Loss: 0.633\t Valid Accuracy: 83.129\t Step Time: 33.87  \n",
      "Epoch: 21/30\t Train Loss: 0.703\t Valid Loss: 0.633\t Valid Accuracy: 83.129\t Step Time: 33.25  \n",
      "Epoch: 21/30\t Train Loss: 0.706\t Valid Loss: 0.633\t Valid Accuracy: 83.129\t Step Time: 34.21  \n",
      "Epoch: 21/30\t Train Loss: 0.686\t Valid Loss: 0.633\t Valid Accuracy: 83.129\t Step Time: 34.04  \n",
      "Epoch: 22/30\t Train Loss: 0.705\t Valid Loss: 0.633\t Valid Accuracy: 83.129\t Step Time: 33.26  \n",
      "Epoch: 22/30\t Train Loss: 0.687\t Valid Loss: 0.633\t Valid Accuracy: 83.129\t Step Time: 33.93  \n",
      "Epoch: 22/30\t Train Loss: 0.728\t Valid Loss: 0.514\t Valid Accuracy: 84.691\t Step Time: 47.22  (c)\n",
      "Epoch: 23/30\t Train Loss: 0.717\t Valid Loss: 0.514\t Valid Accuracy: 84.691\t Step Time: 33.30  \n",
      "Epoch: 23/30\t Train Loss: 0.702\t Valid Loss: 0.514\t Valid Accuracy: 84.691\t Step Time: 33.93  \n",
      "Epoch: 23/30\t Train Loss: 0.638\t Valid Loss: 0.514\t Valid Accuracy: 84.691\t Step Time: 34.01  \n",
      "Epoch: 23/30\t Train Loss: 0.737\t Valid Loss: 0.514\t Valid Accuracy: 84.691\t Step Time: 33.88  \n",
      "Epoch: 24/30\t Train Loss: 0.700\t Valid Loss: 0.514\t Valid Accuracy: 84.691\t Step Time: 33.14  \n",
      "Epoch: 24/30\t Train Loss: 0.676\t Valid Loss: 0.514\t Valid Accuracy: 84.691\t Step Time: 34.12  \n",
      "Epoch: 24/30\t Train Loss: 0.693\t Valid Loss: 0.514\t Valid Accuracy: 84.691\t Step Time: 33.95  \n",
      "Epoch: 25/30\t Train Loss: 0.631\t Valid Loss: 0.514\t Valid Accuracy: 84.691\t Step Time: 33.37  \n",
      "Epoch: 25/30\t Train Loss: 0.701\t Valid Loss: 0.514\t Valid Accuracy: 84.691\t Step Time: 33.86  \n",
      "Epoch: 25/30\t Train Loss: 0.680\t Valid Loss: 0.525\t Valid Accuracy: 85.653\t Step Time: 47.21  (c)\n",
      "Epoch: 26/30\t Train Loss: 0.714\t Valid Loss: 0.525\t Valid Accuracy: 85.653\t Step Time: 33.34  \n",
      "Epoch: 26/30\t Train Loss: 0.657\t Valid Loss: 0.525\t Valid Accuracy: 85.653\t Step Time: 34.09  \n",
      "Epoch: 26/30\t Train Loss: 0.617\t Valid Loss: 0.525\t Valid Accuracy: 85.653\t Step Time: 33.85  \n",
      "Epoch: 27/30\t Train Loss: 0.714\t Valid Loss: 0.525\t Valid Accuracy: 85.653\t Step Time: 33.28  \n",
      "Epoch: 27/30\t Train Loss: 0.688\t Valid Loss: 0.525\t Valid Accuracy: 85.653\t Step Time: 33.91  \n",
      "Epoch: 27/30\t Train Loss: 0.650\t Valid Loss: 0.525\t Valid Accuracy: 85.653\t Step Time: 34.02  \n",
      "Epoch: 28/30\t Train Loss: 0.667\t Valid Loss: 0.525\t Valid Accuracy: 85.653\t Step Time: 33.49  \n",
      "Epoch: 28/30\t Train Loss: 0.652\t Valid Loss: 0.525\t Valid Accuracy: 85.653\t Step Time: 33.92  \n",
      "Epoch: 28/30\t Train Loss: 0.630\t Valid Loss: 0.525\t Valid Accuracy: 85.653\t Step Time: 34.03  \n",
      "Epoch: 28/30\t Train Loss: 0.650\t Valid Loss: 0.441\t Valid Accuracy: 87.456\t Step Time: 47.27  (c)\n",
      "Epoch: 29/30\t Train Loss: 0.709\t Valid Loss: 0.441\t Valid Accuracy: 87.456\t Step Time: 33.07  \n",
      "Epoch: 29/30\t Train Loss: 0.657\t Valid Loss: 0.441\t Valid Accuracy: 87.456\t Step Time: 33.82  \n",
      "Epoch: 29/30\t Train Loss: 0.658\t Valid Loss: 0.441\t Valid Accuracy: 87.456\t Step Time: 33.95  \n",
      "Epoch: 30/30\t Train Loss: 0.648\t Valid Loss: 0.441\t Valid Accuracy: 87.456\t Step Time: 33.36  \n",
      "Epoch: 30/30\t Train Loss: 0.671\t Valid Loss: 0.441\t Valid Accuracy: 87.456\t Step Time: 33.88  \n",
      "Epoch: 30/30\t Train Loss: 0.559\t Valid Loss: 0.441\t Valid Accuracy: 87.456\t Step Time: 33.99  \n"
     ]
    }
   ],
   "source": [
    "# 进行训练\n",
    "train(model.to(device), trainloader, testloader, criterion, optimizer, print_step=32, epochs=30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试网络\n",
    "\n",
    "建议使用网络在训练或验证过程中从未见过的测试数据测试训练的网络。这样，可以很好地判断模型预测全新图像的效果。用网络预测测试图像，并测量准确率，就像验证过程一样。如果模型训练良好的话，你应该能够达到大约 70% 的准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Do validation on the test set\n",
    "# 定义测试网络\n",
    "def test(model, criterion, testloader):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tloss, taccuracy = validation(model, criterion, testloader)\n",
    "\n",
    "    print('Test Accuracy: {:.3f}'.format(taccuracy/len(testloader)*100))\n",
    "\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.160\n"
     ]
    }
   ],
   "source": [
    "# 进行测试\n",
    "test(model.to(device), criterion, testloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存检查点\n",
    "\n",
    "训练好网络后，保存模型，以便稍后加载它并进行预测。你可能还需要保存其他内容，例如从类别到索引的映射，索引是从某个图像数据集中获取的：`image_datasets['train'].class_to_idx`。你可以将其作为属性附加到模型上，这样稍后推理会更轻松。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "": "",
     "classes": [],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 注意，稍后你需要完全重新构建模型，以便用模型进行推理。确保在检查点中包含你所需的任何信息。如果你想加载模型并继续训练，则需要保存周期数量和优化器状态 `optimizer.state_dict`。你可能需要在下面的下个部分使用训练的模型，因此建议立即保存它。\n",
    "\n",
    "\n",
    "# ```python\n",
    "# TODO: Save the checkpoint  \n",
    "# 保存检查点数据\n",
    "checkpoint = {\n",
    "    'input_size': input_size,\n",
    "    'output_size': output_size,\n",
    "    'hidden_layer_size': [each.out_features for each in model.classifier.hidden_layers],\n",
    "    'drop_p': drop_p,\n",
    "    'epochs': epochs,\n",
    "    'optimizer_state': optimizer.state_dict(),\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model': 'densenet121',\n",
    "    'class_to_idx': train_datasets.class_to_idx\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, \n",
    "           'checkpoint_{}_bs=64_2.pth'.format(\n",
    "               \"_\".join([str(each.out_features) for each in  model.classifier.hidden_layers])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载检查点\n",
    "\n",
    "此刻，建议写一个可以加载检查点并重新构建模型的函数。这样的话，你可以回到此项目并继续完善它，而不用重新训练网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO: Write a function that loads a checkpoint and rebuilds the model、\n",
    "# 加载检查点数据\n",
    "def load_checkpoint(filepath, drop_p=None):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = models.densenet121(pretrained=True)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    model.classifier = classifier(checkpoint['input_size'],\n",
    "                                 checkpoint['output_size'],\n",
    "                                 checkpoint['hidden_layer_size'],\n",
    "                                 drop_p if drop_p else checkpoint['drop_p'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.class_to_idx = checkpoint['class_to_idx']\n",
    "    \n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "    \n",
    "    for state in optimizer.state.values():\n",
    "        for k, v in state.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                state[k] = v.cuda()\n",
    "    \n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 类别推理\n",
    "\n",
    "现在，你需要写一个使用训练的网络进行推理的函数。即你将向网络中传入一个图像，并预测图像中的花卉类别。写一个叫做 `predict` 的函数，该函数会接受图像和模型，然后返回概率在前 $K$ 的类别及其概率。应该如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)\n",
    "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
    "> ['70', '3', '45', '62', '55']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，你需要处理输入图像，使其可以用于你的网络。\n",
    "\n",
    "## 图像处理\n",
    "\n",
    "你需要使用 `PIL` 加载图像（[文档](https://pillow.readthedocs.io/en/latest/reference/Image.html)）。建议写一个函数来处理图像，使图像可以作为模型的输入。该函数应该按照训练的相同方式处理图像。\n",
    "\n",
    "首先，调整图像大小，使最小的边为 256 像素，并保持宽高比。为此，可以使用 [`thumbnail`](http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail) 或 [`resize`](http://pillow.readthedocs.io/en/3.1.x/reference/Image.html#PIL.Image.Image.thumbnail) 方法。然后，你需要从图像的中心裁剪出 224x224 的部分。\n",
    "\n",
    "图像的颜色通道通常编码为整数 0-255，但是该模型要求值为浮点数 0-1。你需要变换值。使用 Numpy 数组最简单，你可以从 PIL 图像中获取，例如 `np_image = np.array(pil_image)`。\n",
    "\n",
    "和之前一样，网络要求图像按照特定的方式标准化。均值应标准化为 `[0.485, 0.456, 0.406]`，标准差应标准化为 `[0.229, 0.224, 0.225]`。你需要用每个颜色通道减去均值，然后除以标准差。\n",
    "\n",
    "最后，PyTorch 要求颜色通道为第一个维度，但是在 PIL 图像和 Numpy 数组中是第三个维度。你可以使用 [`ndarray.transpose`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ndarray.transpose.html)对维度重新排序。颜色通道必须是第一个维度，并保持另外两个维度的顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行图像处理\n",
    "def process_image(image):\n",
    "    ''' Scales, crops, and normalizes a PIL image for a PyTorch model,\n",
    "        returns an Numpy array\n",
    "    '''\n",
    "    \n",
    "    # TODO: Process a PIL image for use in a PyTorch model\n",
    "    img = Image.open(image)\n",
    "    \n",
    "    tsize = (256, 256)\n",
    "    img.thumbnail(tsize)\n",
    "\n",
    "    lwsize = (img.size[0] - 224)/2\n",
    "    thsize = (img.size[1] - 224)/2\n",
    "    rwsize = (img.size[0] + 224)/2\n",
    "    bhsize = (img.size[1] + 224)/2\n",
    "    \n",
    "    img = img.crop((lwsize, thsize, rwsize, bhsize))\n",
    "    \n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    \n",
    "    npimg = np.array(img)\n",
    "    npimg = npimg/255\n",
    "    \n",
    "    npimg = (npimg - mean)/std\n",
    "    \n",
    "    npimg = npimg.transpose((2,0,1))\n",
    "\n",
    "    return torch.from_numpy(npimg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要检查你的项目，可以使用以下函数来转换 PyTorch 张量并将其显示在  notebook 中。如果 `process_image` 函数可行，用该函数运行输出应该会返回原始图像（但是剪裁掉的部分除外）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像大小\n",
    "def imshow(image, ax=None, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    \n",
    "    # pyTorch tensors中颜色通道为一维 matplotlib中为三维\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    # 撤消预处理\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = std * image + mean\n",
    "    \n",
    "    # 图像剪裁范围为区间（0，1），或者当其显示时看起来像噪声时也需要剪裁\n",
    "    image = np.clip(image, 0, 1)\n",
    "    \n",
    "    ax.imshow(image)\n",
    "    \n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 类别预测\n",
    "\n",
    "可以获得格式正确的图像后 \n",
    "\n",
    "要获得前 $K$ 个值，在张量中使用 [`x.topk(k)`](http://pytorch.org/docs/master/torch.html#torch.topk)。该函数会返回前 `k` 个概率和对应的类别索引。你需要使用  `class_to_idx`（希望你将其添加到了模型中）将这些索引转换为实际类别标签，或者从用来加载数据的[ `ImageFolder`](https://pytorch.org/docs/master/torchvision/datasets.html?highlight=imagefolder#torchvision.datasets.ImageFolder)进行转换。确保颠倒字典\n",
    "\n",
    "同样，此方法应该接受图像路径和模型检查点，并返回概率和类别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs, classes = predict(image_path, model)\n",
    "print(probs)\n",
    "print(classes)\n",
    "> [ 0.01558163  0.01541934  0.01452626  0.01443549  0.01407339]\n",
    "> ['70', '3', '45', '62', '55']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image_path, model, topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    \n",
    "    # TODO: Implement the code to predict the class from an image file\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    imaget = process_image(image_path)\n",
    "    imaget.to(device)\n",
    "    \n",
    "    imaget = imaget.unsqueeze(0)\n",
    "    image = imaget.type(torch.cuda.FloatTensor)\n",
    "    output = model.forward(image)\n",
    "    ps = torch.exp(output)\n",
    "    model.train()\n",
    "    return ps.topk(topk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
     ]
    }
   ],
   "source": [
    "model, optimizer = load_checkpoint('checkpoint_512_bs=64_2.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查运行状况\n",
    "\n",
    "你已经可以使用训练的模型做出预测，现在检查模型的性能如何。即使测试准确率很高，始终有必要检查是否存在明显的错误。使用 `matplotlib` 将前 5 个类别的概率以及输入图像绘制为条形图，应该如下所示：\n",
    "\n",
    "<img src='assets/inference_example.png' width=300px>\n",
    "\n",
    "你可以使用 `cat_to_name.json` 文件（应该之前已经在 notebook 中加载该文件）将类别整数编码转换为实际花卉名称。要将 PyTorch 张量显示为图像，请使用定义如下的 `imshow` 函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f52d0b418d0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Display an image along with the top 5 classes\n",
    "image_path = 'flowers/test/21/image_06807.jpg'\n",
    "\n",
    "probs, labels = predict(image_path, model)\n",
    "\n",
    "ps = [x for x in probs.cpu().detach().numpy()[0]]\n",
    "npar = [x for x in labels.cpu().numpy()[0]]\n",
    "names = list()\n",
    "\n",
    "inv_mapping = {v: k for k, v in model.class_to_idx.items()}\n",
    "\n",
    "for i in npar:\n",
    "    names.append(cat_to_name[str(inv_mapping[i])])\n",
    "\n",
    "\n",
    "imshow(process_image(image_path), ax=plt.subplot(2,1,1));\n",
    "plt.title(cat_to_name['21'])\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "sb.barplot(y=names, x=ps, color=sb.color_palette()[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
